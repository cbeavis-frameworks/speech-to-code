# SpeechToCode Multi-Agent Implementation Plan

## Overview

This implementation plan outlines the steps needed to enhance the SpeechToCode application with a multi-agent architecture that integrates OpenAI's Realtime API and Anthropic's Claude Code CLI. The plan builds upon the existing Terminal Controller feature to create a seamless voice-controlled coding experience with two AI agents:

1. **Conversation Agent**: Handles user interaction, orchestrates workflow, and manages communication with the Terminal/Claude
2. **Planning Agent**: Maintains project context, tracks tasks, and provides long-term memory for the system

## Phase 1: Foundation Setup

### 1.1. Project Configuration 
- [x] Add OpenAI API client dependencies
- [x] Configure API keys and authentication for OpenAI
- [x] Set up environment variables for API access
- [x] Update project entitlements for necessary permissions

### 1.2. Terminal Controller Enhancements 
- [x] Extend TerminalController to support Claude Code CLI-specific commands
- [x] Add detection for Claude Code interactive prompts
- [x] Implement Claude Code command history tracking
- [x] Create specialized response parsing for Claude Code output

### 1.3. AI Agent Models 
- [x] Create RealtimeSession model for managing OpenAI Realtime API sessions
- [x] Implement ConversationAgent model with state management
- [x] Implement PlanningAgent model with persistent storage
- [x] Create AgentMessage model for structured communication

## Phase 2: Voice Processing Integration

The goal of this phase is to add voice processing capabilities to the SpeechToCode application.

### 2.1 Voice Processing Design (COMPLETE)

- Design a `VoiceProcessor` class to handle audio input and output
- Determine the architecture for integrating voice with the existing agent system
- Ensure compatibility with the multi-agent architecture from Phase 1

### 2.2 Voice Processing Implementation (COMPLETE)

- [x] Implement the `VoiceProcessor` class with the following capabilities:
  - [x] Use macOS Speech Recognition framework to convert speech to text locally
  - [x] Provide voice activity detection (VAD) to determine when the user is speaking
  - [x] Manage speech recognition sessions and transcription processing
  - [x] Support text-to-speech for AI responses
  
- [x] Modify the `RealtimeSession` to work with transcribed text rather than raw audio:
  - [x] Remove audio streaming functionality
  - [x] Add methods for processing transcriptions from the VoiceProcessor
  - [x] Ensure proper handler delegation for transcription updates
  
- [x] Update the `ConversationAgent` to manage voice command processing:
  - [x] Add methods for starting and stopping voice listening
  - [x] Implement delegate methods for handling transcription updates
  - [x] Process recognized speech for command execution

- [x] Update the `AgentMessage` model to support voice-related message types:
  - [x] Add voice input message type for transcribed user speech
  - [x] Add voice output message type for AI responses to be spoken

## Phase 3: Planning Agent Implementation

### 3.1. Plan Storage
- [x] Create PlanStorage model for persistent plan data
- [x] Implement file-based or database storage for plans
- [x] Add serialization/deserialization for plan data
- [x] Create backup and recovery mechanisms

### 3.2. Plan Management
- [x] Implement plan creation and initialization
- [x] Add task tracking and status updates
- [x] Create plan summarization functionality
- [x] Implement plan versioning and history

### 3.3. Agent Communication
- [x] Set up communication channel between Conversation and Planning agents
- [x] Implement structured responses via Realtime API
- [x] Create structured message format for agent-to-agent communication
- [x] Add message routing and handling

## Phase 4: Claude Code Integration

### 4.1. Claude CLI Setup
- [x] Ensure Claude Code CLI is properly installed and configured
- [x] Create initialization script for Claude Code session
- [x] Implement Claude Code session management
- [x] Add authentication handling for Claude Code

### 4.2. Command Routing
- [x] Enhance TerminalController to route commands to Claude Code
- [x] Implement specialized command formatting for Claude Code
- [x] Add response parsing for Claude Code output
- [x] Create interactive prompt handling for Claude Code

### 4.3. Automated Decision Making
- [ ] Implement decision tree for common Claude Code prompts
- [ ] Create context-aware auto-response system
- [ ] Add user confirmation for critical decisions
- [ ] Implement fallback to user input when needed

## Phase 5: Integration and Workflow

### 5.1. Multi-Agent Orchestration
- [ ] Implement startup sequence for both agents
- [ ] Create session management for Realtime API connections
- [ ] Add coordinated shutdown and cleanup
- [ ] Implement error handling and recovery

### 5.2. Context Management
- [ ] Create context sharing between agents
- [ ] Implement project context initialization
- [ ] Add context refreshing mechanisms
- [ ] Create context summarization for efficient token usage

### 5.3. Workflow Automation
- [ ] Implement common workflow patterns
- [ ] Create task execution sequences
- [ ] Add progress tracking and reporting
- [ ] Implement workflow customization options

### 5.4. User Experience Refinement
- [ ] Add visual feedback for agent actions
- [ ] Implement user preference settings
- [ ] Create help and documentation system
- [ ] Add onboarding experience for new users

## Phase 6: Testing and Optimization

### 6.1. Unit Testing
- [ ] Create tests for individual agent components
- [ ] Implement terminal interaction tests
- [ ] Add API communication tests
- [ ] Create model validation tests

### 6.2. Integration Testing
- [ ] Test multi-agent communication
- [ ] Verify terminal command execution
- [ ] Test Claude Code interaction
- [ ] Validate speech recognition and transcription

### 6.3. Performance Optimization
- [ ] Optimize token usage for APIs
- [ ] Improve response time for speech recognition
- [ ] Enhance terminal command execution speed
- [ ] Optimize context management for efficiency

### 6.4. Error Handling
- [ ] Implement comprehensive error recovery
- [ ] Add logging for debugging
- [ ] Create user-friendly error messages
- [ ] Implement fallback mechanisms

## Implementation Specifications

### Voice Processing

The voice processing implementation will use the following approach:

1. Use macOS's built-in Speech Recognition framework for speech-to-text conversion
   - This utilizes Apple's high-quality speech recognition technology
   - Processing happens locally on the device
   - Supports multiple languages and dialects

2. Voice Activity Detection (VAD)
   - Monitor audio levels to detect when the user starts and stops speaking
   - Automatically stop recording after a period of silence
   - Provide visual feedback about voice detection status

3. Speech Recognition Process Flow
   - When user starts speaking, begin a speech recognition session
   - As user speaks, update UI with interim transcription
   - When silence is detected, finalize the transcription
   - Send the transcribed text to the Realtime API for processing

4. Text-to-Speech for AI Responses
   - Use AVSpeechSynthesizer for converting AI responses to speech
   - Control speech rate, pitch, and volume for natural-sounding responses

### Realtime API Integration

The Realtime API integration will now use the following design:

1. Connection Management
   - Establish and maintain WebSocket connection to the OpenAI Realtime API
   - Handle reconnection in case of connection loss
   - Manage authentication with the API key

2. Message Processing
   - Process text-based messages from transcribed speech
   - Handle delta responses from the API (text chunks, function calls)
   - Support streaming responses for real-time interaction

3. State Management
   - Track session state (connecting, connected, disconnected)
   - Manage voice processing state (listening, processing)
   - Coordinate with the ConversationAgent for overall application flow

### Agent Communication Flow

1. User speaks a command â†’ macOS Speech Recognition converts to text
2. Transcribed text sent to Conversation Agent via Realtime API
3. Conversation Agent processes request and may:
   - Query Planning Agent for context
   - Issue terminal commands via function calling
   - Generate direct responses to the user
4. Terminal Controller executes commands in Terminal (with Claude Code)
5. Terminal output is captured and returned to Conversation Agent
6. Conversation Agent decides next steps based on terminal output and plan
7. Agent provides text response, which may be synthesized to speech for user feedback

### Data Persistence

- Project plans stored in structured JSON format
- Session history maintained for context continuity
- Terminal command history tracked for reference
- User preferences saved for personalization